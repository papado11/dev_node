# 17 误差的计算

## 17.1 MSE

均方差  
$MSE=\frac{1}{N}\sum(\hat{y_i}-y_i)^2$  

```python
y = tf.constant([1, 2, 3, 0, 2])
y = tf.one_hot(y, depth=4)
# <tf.Tensor: shape=(5, 4), dtype=float32, numpy=
# array([[0., 1., 0., 0.],
#        [0., 0., 1., 0.],
#        [0., 0., 0., 1.],
#        [1., 0., 0., 0.],
#        [0., 0., 1., 0.]], dtype=float32)>
out = tf.random.normal([5, 4])
loss1 = tf.reduce_mean(tf.square(y-out))
loss2 = tf.square(tf.norm(y-out))/(5*4)
loss3 = tf.reduce_mean(tf.losses.MSE(y, out))
loss1, loss2, loss3
# (<tf.Tensor: shape=(), dtype=float32, numpy=1.1351672>,
#  <tf.Tensor: shape=(), dtype=float32, numpy=1.1351674>,
#  <tf.Tensor: shape=(), dtype=float32, numpy=1.1351672>)
```

## 17.2 Entropy

熵: $H(X)=-\sum\limits_{i=1}^n p(x_i)\log_2 (p(x_i))$  
交叉熵：  
$H(p,q)=-\sum\limits_{i=1}^nq(x_i)\log_2 (p(x_i))$  
（以下计算都使用自然对数，Tensorflow中都是自然对数）  
对于预测值:
Q=[0.4, 0.3, 0.05, 0.2]  
实际值:  
P=[1, 0, 0, 0]
交叉熵:  
$H =-(1\ln 4+0\ln 0.3+0\ln 0.05+0\ln 0.2)=-\ln 0.4\approx0.916$  
对于二分类问题的交叉：  
Q=0.1,P=1 时：$H =-(1\ln 0.1)\approx2.3025842$  
Q=0.1,P=0 时：$H =-(0*\ln 0.1+1\ln (1-0.1))\approx0.10536041$  
损失函数:  
$Loss=\frac{1}{N}H$

```python
a = tf.constant(0.1)
-tf.math.log(a)/tf.math.log(2.) # 计算熵
# <tf.Tensor: shape=(), dtype=float32, numpy=3.321928>
a = tf.constant([0.1, 0.1, 0.1, 0.7])
-tf.reduce_sum(a*tf.math.log(a)/tf.math.log(2.)) # 计算熵
# <tf.Tensor: shape=(), dtype=float32, numpy=1.3567796>
a = tf.constant([0.01, 0.01, 0.01, 0.97])
-tf.reduce_sum(a*tf.math.log(a)/tf.math.log(2.)) # 计算熵
# <tf.Tensor: shape=(), dtype=float32, numpy=0.24194068>
a = tf.fill([4], 0.25)
-tf.reduce_sum(a*tf.math.log(a)/tf.math.log(2.)) # 计算熵
# <tf.Tensor: shape=(), dtype=float32, numpy=2.0>

tf.losses.binary_crossentropy([0], [0.1]) # 二分类交叉熵损失函数
# <tf.Tensor: shape=(), dtype=float32, numpy=0.10536041>
tf.losses.binary_crossentropy([0, 1], [0.1, 0.5]) # 二分类交叉熵损失函数
# <tf.Tensor: shape=(), dtype=float32, numpy=0.39925367>

tf.losses.categorical_crossentropy([0, 0, 0, 1], [0.01, 0.01, 0.01, 0.97]) # 多分类交叉熵损失函数
# <tf.Tensor: shape=(), dtype=float32, numpy=0.030459179>

# 在计算多分类问题时,推荐最后输出直接用来计算loss, 而不是通过softmax后再计算loss
# categorical_crossentropy在内部会优化一些NAN,INF的问题.
tf.losses.categorical_crossentropy([0, 1], logits, from_logits=True)

#不推荐这么做
tf.losses.categorical_crossentropy([0, 1, prob)
```
