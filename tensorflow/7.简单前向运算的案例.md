#

```python
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, optimizers

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

(x, y), (x_test, y_test) = datasets.mnist.load_data()
x = tf.convert_to_tensor(x, dtype=tf.float32) / 255
y = tf.convert_to_tensor(y, dtype=tf.uint8)
# y = tf.one_hot(y, depth=10)
x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255
y_test = tf.convert_to_tensor(y_test, dtype=tf.uint8)
# y_test = tf.one_hot(y_test, depth=10)
x.shape, y.shape, x_test.shape, y_test.shape
# (TensorShape([60000, 28, 28]),
#  TensorShape([60000]),
#  TensorShape([10000, 28, 28]),
#  TensorShape([10000]))
train_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(200)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(200)

W1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
B1 = tf.Variable(tf.random.truncated_normal([256], stddev=0.1))
W2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))
B2 = tf.Variable(tf.random.truncated_normal([128], stddev=0.1))
W3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))
B3 = tf.Variable(tf.random.truncated_normal([10], stddev=0.1))

lr = 1e-3

def model(x):
    yp = x @ W1 + B1
    yp = tf.nn.relu(yp)
    yp = yp @ W2 + B2
    yp = tf.nn.relu(yp)
    yp = yp @ W3 + B3
    yp = tf.nn.softmax(yp, axis=1)
    return yp

def update_variable(v: tf.Variable, grad):
    v.assign_sub(lr * grad)

def update_variables(grads):
    update_variable(W1, grads[0])
    update_variable(B1, grads[1])
    update_variable(W2, grads[2])
    update_variable(B2, grads[3])
    update_variable(W3, grads[4])
    update_variable(B3, grads[5])

def train(epoch):
    for step, (x, y) in enumerate(train_dataset):
        x = tf.reshape(x, [-1, 28 * 28])
        y = tf.one_hot(y, depth=10)
        with tf.GradientTape() as tape:
            # yp = x @ W1 + B1
            # yp = tf.nn.relu(yp)
            # yp = yp @ W2 + B2
            # yp = tf.nn.relu(yp)
            # yp = yp @ W3 + B3
            # yp = tf.nn.softmax(yp, axis=1)
            yp = model(x)

            # loss
            loss = tf.square(yp - y)
            # print(loss.shape)(200, 10)
            loss = tf.reduce_mean(loss)

        grads = tape.gradient(loss, [W1, B1, W2, B2, W3, B3])
        update_variables(grads)

        loss = float(loss)
        if step % 100 == 0:
            print(epoch, step, "loss->", loss)
            print(f'epoch: {epoch}, step: {step}, loss: {float(loss)}.')

    return loss

def test(epoch, loss):
    total_correct, total_num = 0, 0
    for step, (x, y) in enumerate(test_dataset):
        x = tf.reshape(x, [-1, 28 * 28])
        out = model(x)
        pred = tf.argmax(out, axis=1)
        pred = tf.cast(pred, dtype=tf.uint8)
        correct = tf.cast(tf.equal(pred, y), dtype=tf.uint8)
        correct = tf.reduce_sum(correct)

        total_correct += int(correct)
        total_num += x.shape[0]

    acc = total_correct / total_num
    print(f'epoch: {epoch}, loss: {loss}, test: acc: {acc}.')

def train_epoch(epoch_num):
    for epoch in range(5):
        loss = train(epoch)
        test(epoch, loss)

# lr = 1e-1
train_epoch(5)
```
