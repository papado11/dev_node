# 25.fashion数据集实战

```python
import os

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, datasets, layers, metrics, optimizers
from datetime import datetime

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()
x.shape, y.shape, x_test.shape, y_test.shape

batchsz = 128

def preprocess(x, y):
    x = tf.cast(x, dtype=tf.float32) / 255.0
    y = tf.cast(y, dtype=tf.int32)
    return x, y

db = tf.data.Dataset.from_tensor_slices((x, y))
db = db.map(preprocess).shuffle(10000).batch(batchsz)

db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))
db_test = db_test.map(preprocess).shuffle(10000).batch(batchsz)

model = Sequential(
    [
        layers.Dense(256, activation=tf.nn.relu),
        layers.Dense(128, activation=tf.nn.relu),
        layers.Dense(64, activation=tf.nn.relu),
        layers.Dense(32, activation=tf.nn.relu),
        layers.Dense(10, activation=tf.nn.relu),
    ]
)
model.build(input_shape=[None, 28 * 28])

optimizer = optimizers.Adam(learning_rate=1e-3)

##### TensorBoard
current_time = datetime.now().strftime("%Y%m%d-%H%M%S")
log_dir = f"./logdir/{current_time}"
summary_writer = tf.summary.create_file_writer(log_dir)

epoch_start = 0

def train_epoch(epoch: int):
    loss_sum = 0
    x_count = 0 # num of sample
    for step, (x, y) in enumerate(db):
        x = tf.reshape(x, [-1, 28 * 28])
        x_count += x.shape[0] # display element
        y_onehot = tf.one_hot(y, depth=10)
        with tf.GradientTape() as tape:
            logits = model(x)
            # loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))
            loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)
            loss_sum += float(tf.reduce_sum(loss_ce)) # display element
            loss_ce = tf.reduce_mean(loss_ce)

        grads = tape.gradient(loss_ce, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        loss = float(loss_ce)
        if step % 100 == 0:
            print(f"[{epoch}]({step})> loss_ce: {loss}.")

    loss = loss_sum / x_count

    ## test
    total_corrent = 0
    total_num = 0
    for x, y in db_test:
        x = tf.reshape(x, [-1, 28 * 28])
        logits = model(x)  # [b, 10]
        prob = tf.nn.softmax(logits, axis=1)
        pred = tf.argmax(prob, axis=1)  # the index of max value in prob's axis=1 => [b]
        pred = tf.cast(pred, dtype=tf.int32)
        corrent = tf.equal(pred, y)  # [b], True: equal, False: not equal
        corrent = tf.reduce_sum(tf.cast(corrent, dtype=tf.int32))

        total_corrent += int(corrent)
        total_num += x.shape[0]

    acc = round(total_corrent / total_num, 4)

    print(f"[{epoch}]> test acc: {acc}, loss: {loss}")
    with summary_writer.as_default():
        tf.summary.scalar('loss', loss, step=epoch)
        tf.summary.scalar('test acc', acc, step=epoch)

def train(epochs: int):
    global epoch_start
    for epoch in range(epoch_start, epoch_start + epochs):
        train_epoch(epoch)

    epoch_start+=epochs

train(5)
```
